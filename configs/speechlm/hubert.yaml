dataset:
  wav_dir_train: "data/librilight"
  ext_audio: ".flac"

  train: "ryota-komatsu/librilight"
  units_per_sample: 125

  swuggy: "ryota-komatsu/swuggy"  # lexical
  sblimp: "ryota-komatsu/sblimp"  # syntactic

  APP_DIR: "data/zr-data"
  result_dir: "results/speechlm/hubert"

dataloader:
  batch_size_per_device: 1000  # effective batch size (tokens) = dataset.units_per_sample * batch_size_per_device * #GPUs

model:
  name: "microsoft/Phi-4-mini-reasoning"
  path: "models/speechlm/hubert"
  vocab_size: ${s2u.vocab_size}

  lora:
    r: 32
    lora_alpha: 8
    target_modules: "all-linear"

optim:
  epoch: 24
  warmup_steps: 100
  lr: 0.0005
  lr_min: 0.00005
  beta1: 0.9
  beta2: 0.98
  max_norm: 0.1
  summary_interval: 100
  validation_save_interval: 10000
  total_steps: 200000

s2u:
  dense_model_name: "hubert-base-ls960"
  quantizer_model_name: "kmeans"
  vocab_size: 100
  num_workers: 16