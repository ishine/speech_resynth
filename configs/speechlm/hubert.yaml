dataset:
  wav_dir_train: "data/librilight"
  ext_audio: ".flac"

  train: "ryota-komatsu/librilight"
  units_per_sample: 125

  swuggy: "ryota-komatsu/swuggy"  # lexical
  sblimp: "ryota-komatsu/sblimp"  # syntactic

  APP_DIR: "data/zr-data"
  result_dir: "results/speechlm/hubert"

dataloader:
  batch_size_per_device: 1000  # effective batch size (tokens) = dataset.units_per_sample * batch_size_per_device * #GPUs

model:
  path: "models/speechlm/hubert"
  vocab_size: ${s2u.vocab_size}
  hidden_size: 768
  intermediate_size: 2048  # 4 * hidden_size * 2 / 3
  num_hidden_layers: 12
  num_attention_heads: 12
  pad_token_id: 0
  bos_token_id: null
  eos_token_id: 1  # for generation stopping criteria
  tie_word_embeddings: true

optim:
  epoch: 24
  warmup_steps: 100
  lr: 0.0005
  lr_min: 0.00005
  beta1: 0.9
  beta2: 0.98
  max_norm: 0.1
  summary_interval: 100
  validation_save_interval: 10000
  total_steps: 200000

s2u:
  dense_model_name: "hubert-base-ls960"
  quantizer_model_name: "kmeans"
  vocab_size: 100
  num_workers: 16