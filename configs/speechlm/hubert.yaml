dataset:
  wav_dir_train: "data/librilight"
  ext_audio: ".flac"

  unicode_train: "data/speechlm/hubert/unicode/train"
  train_file: "data/speechlm/hubert/unit/train.txt"
  units_per_sample: 125

  swuggy_dev_file: "data/speechlm/hubert/unit/lexical/dev.json"
  sblimp_dev_file: "data/speechlm/hubert/unit/syntactic/dev.json"
  swuggy_test_file: "data/speechlm/hubert/unit/lexical/test.json"
  sblimp_test_file: "data/speechlm/hubert/unit/syntactic/test.json"

  APP_DIR: "data/zr-data"
  result_dir: "results/speechlm/hubert"

dataloader:
  batch_size_per_device: 960  # effective batch size (tokens) = dataset.units_per_sample * batch_size_per_device * #GPUs

model:
  path: "models/speechlm/hubert"
  vocab_size: 8192  # BPE vocab size
  hidden_size: 768
  intermediate_size: 2048  # 4 * hidden_size * 2 / 3
  num_hidden_layers: 12
  num_attention_heads: 12
  pad_token_id: 0
  bos_token_id: null
  eos_token_id: 1  # for generation stopping criteria

optim:
  epoch: 23
  warmup_steps: 100
  lr: 0.0005
  lr_min: 0.00005
  beta1: 0.9
  beta2: 0.98
  max_norm: 1.0
  summary_interval: 100
  validation_save_interval: 10000
  total_steps: 200000

s2u:
  dense_model_name: "hubert-base-ls960"
  quantizer_model_name: "kmeans"
  vocab_size: 100

  tokenizer_path: "models/speechlm/hubert/tokenizer.json"

  num_workers: 16