<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        :root {
            --c0: #dedede;
        }

        td {
            vertical-align: middle;
        }

        audio {
            width: 20vw;
            min-width: 100px;
            max-width: 250px;
        }

        .float-div {
            box-shadow: 0 0.5rem 1rem var(--c0);
            padding: 3rem;
            margin-top: 3rem;
            margin-bottom: 3rem;
            border-radius: 0.5rem;
            background: white;
        }

        p {
            text-align: justify;
        }
    </style>
</head>

<div class="container float-div">
    <div class="text-center">
        <h3>Comparison of resynthesized speech between phonetic tokens and acoustic tokens</h3>
        <h6>
            [<a href="https://github.com/ryota-komatsu/speech_resynth" , target='_blank'>Code</a>]
            [<a href="https://huggingface.co/ryota-komatsu/flow_matching_with_bigvgan" , target='_blank'>Model</a>]
        </h6>
    </div>

    <p style="text-align: center; font-style: italic;">
        Ryota Komatsu
    </p>
    <p style="text-align: center;">
        Institute of Science Tokyo
    </p>
</div>

<div class="container float-div">
    <div class="text-center">
        <h3>Speech resynthesis samples from LibriTTS-R test set</h3>
    </div>
    <div class="table-responsive pt-3">
        <table style="text-align: center" class="table table-hover">
            <thead>
                <tr>
                    <th style="text-align: center; vertical-align: top" rowspan="2">Original</th>
                    <th style="text-align: center; vertical-align: top" rowspan="2">Acoustic token</th>
                    <th style="text-align: center" colspan="2">Phonetic token</th>
                </tr>
                <tr>
                    <th style="text-align: center">sampling #1</th>
                    <th style="text-align: center">sampling #2</th>
                </tr>
            </thead>
            <tbody>

                <tr>
                    <td><audio controls controlslist="nodownload" src='audio/260_123288_000003_000001.wav'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/260_123288_000003_000001_dac.wav'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/260_123288_000003_000001_1.wav'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/260_123288_000003_000001_2.wav'></audio></td>
                </tr>

                <tr>
                    <td><audio controls controlslist="nodownload" src='audio/672_122797_000002_000002.wav'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/672_122797_000002_000002_dac.wav'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/672_122797_000002_000002_1.wav'></audio></td>
                    <td><audio controls controlslist="nodownload" src='audio/672_122797_000002_000002_2.wav'></audio></td>
                </tr>

            </tbody>
        </table>
    </div>
</div>

<div class="container">
    <h3>License</h3>
    <p>
        The
        <a href="https://www.openslr.org/141/" , target='_blank'>LibriTTS-R</a>
        dataset is made available by Google LLC under the
        <a href="http://creativecommons.org/licenses/by/4.0/" , target='_blank'>CC BY-NC 4.0</a>.
    </p>
</div>

<div class="container">
    <h3>References</h3>
    <ol>
        <li>Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka, M. Bacchiani, Y. Zhang, W. Han, and A. Bapna, "LibriTTS-R: A restored multi-speaker text-to-speech corpus," in Proc. Interspeech, 2023, pp. 5496–5500.</li>
        <li>M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, and W.-N. Hsu, "Voicebox: Text-guided multilingual universal speech generation at scale," in Proc. Thirty-seventh Conference on Neural Information Processing Systems, vol. 36, 2023, pp. 14005–14034.</li>
        <li>S. gil Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, "BigVGAN: A universal neural vocoder with large-scale training," in Proc. International Conference on Learning Representations, 2023.</li>
        <li>T. A. Nguyen, W.-N. Hsu, A. D’Avirro, B. Shi, I. Gat, M. Fazel-Zarani, T. Remez, J. Copet, G. Synnaeve, M. Hassid, F. Kreuk, Y. Adi, and E. Dupoux, "Expresso: A benchmark and analysis of discrete expressive speech resynthesis," in Proc. Interspeech, 2023, pp. 4823–4827.</li>
        <li>R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar, "High-Fidelity Audio Compression with Improved RVQGAN," in Proc. NeurIPS, 2023, pp. 27980-27993.</li>
    </ol>
</div>

</html>